# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JgKAFDN6c5CMjxgPrILedz8yh8RXEU4H
"""

# -*- coding: utf-8 -*-
"""
OE-focused ViT training for 3D parcel localization
Key orientation-focused changes:
 - OE-aligned sign-invariant angular loss: theta = arccos(|n . n_hat|)
 - Mask-weighted per-pixel depth normal supervision + aggregated guidance
 - Auxiliary quaternion head and consistency loss with predicted normal
 - OE metric (degrees) and normalized OE_i (theta/theta_max clipped to 1.0)
 - Early stopping & checkpointing based on OE (primary)
 - Maintains ViT + depth-token fusion + segmentation decoder
"""
import os
import random
import math
from pathlib import Path
from typing import Tuple, Dict, Optional

import numpy as np
import pandas as pd
import cv2
from tqdm import tqdm

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

import albumentations as A
from albumentations.pytorch import ToTensorV2

# timm
import timm

# ====================================================================
# CONFIG
# ====================================================================
class Config:
    BASE_DIR = Path('/content/drive/MyDrive/Viettel AI Race/CV')
    TRAIN_DIR = BASE_DIR / 'train'
    IMAGE_SIZE = 224
    NUM_CHANNELS = 4
    NUM_CLASSES = 2
    BATCH_SIZE = 8
    NUM_EPOCHS = 40
    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
    SEED = 42

    # optimizer / scheduler
    LR = 3e-4
    WEIGHT_DECAY = 1e-2
    MAX_LR = 1e-3
    MOMENTUM = 0.9

    # loss weights (tune these for OE)
    WEIGHT_SEG = 1.0
    WEIGHT_CENTER = 1.0
    WEIGHT_NORMAL = 2.0           # stronger emphasis on orientation
    DEPTH_NORMAL_WEIGHT = 0.6     # how much to trust depth normals vs GT normal
    QUAT_LOSS_WEIGHT = 0.5        # auxiliary quaternion consistency loss

    # OE threshold in degrees (same as your evaluation)
    THETA_MAX_DEG = 20.0

    # training tricks
    AMP = True
    GRAD_CLIP = 1.0
    GRAD_ACCUM_STEPS = 1

    USE_ONECYCLE = True
    ONECYCLE_PCT_START = 0.1

    CHECKPOINT_DIR = Path('./checkpoints')
    CHECKPOINT_DIR.mkdir(exist_ok=True)
    BEST_MODEL = CHECKPOINT_DIR / 'best_model.pth'

    # camera intrinsics (your original values)
    K = np.array([
        [650.0616455078125, 0, 649.5928955078125],
        [0, 650.0616455078125, 360.9415588378906],
        [0, 0, 1]
    ], dtype=np.float32)

    ROI = {'x': 560, 'y': 150, 'width': 300, 'height': 330}

# ====================================================================
# UTILITIES
# ====================================================================
def seed_everything(seed=Config.SEED):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

def safe_load_image(path):
    img = cv2.imread(str(path))
    if img is None:
        raise FileNotFoundError(f"Image not found: {path}")
    return img

def depth_to_meters(depth_raw):
    depth = depth_raw.astype(np.float32)
    if depth.max() > 20:
        depth = depth / 1000.0
    return depth

def compute_mask_iou(pred_mask: np.ndarray, gt_mask: np.ndarray, thr=0.5):
    pred = (pred_mask >= thr).astype(np.uint8)
    gt = (gt_mask > 0).astype(np.uint8)
    inter = (pred & gt).sum()
    union = (pred | gt).sum()
    if union == 0:
        return 1.0 if inter == 0 else 0.0
    return inter / union

# OE metric: angle between unit vectors, sign-invariant (abs dot), returns degrees and normalized value (0..1)
def orientation_error_deg_and_normalized(pred_normals: np.ndarray, gt_normals: np.ndarray, theta_max_deg=20.0):
    """
    pred_normals, gt_normals: (B,3) numpy arrays (not necessarily normalized)
    returns: avg_deg, avg_normalized
    """
    eps = 1e-7
    # normalize:
    pn = pred_normals / (np.linalg.norm(pred_normals, axis=1, keepdims=True) + eps)
    gn = gt_normals / (np.linalg.norm(gt_normals, axis=1, keepdims=True) + eps)
    dot = np.sum(pn * gn, axis=1)
    dot_clamped = np.clip(np.abs(dot), -1.0 + 1e-7, 1.0 - 1e-7)
    theta_rad = np.arccos(dot_clamped)
    theta_deg = np.degrees(theta_rad)
    theta_norm = np.minimum(theta_deg / theta_max_deg, 1.0)
    return float(theta_deg.mean()), float(theta_norm.mean())

# ====================================================================
# DATASET
# ====================================================================
class ParcelDataset(Dataset):
    def __init__(self, data_dir, csv_path, transform=None, roi=None, mode='train'):
        self.data_dir = Path(data_dir)
        self.df = pd.read_csv(csv_path)
        self.transform = transform
        self.roi = roi or Config.ROI
        self.mode = mode

        if 'image_filename' in self.df.columns:
            self.df['id'] = self.df['image_filename'].apply(
                lambda x: int(x.replace('image_', '').replace('.png', ''))
            )
        elif 'id' not in self.df.columns:
            self.df['id'] = np.arange(len(self.df))

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        sample_id = f"{row['id']:04d}"

        rgb_path = self.data_dir / 'rgb' / f'{sample_id}.png'
        depth_path = self.data_dir / 'depth' / f'{sample_id}.png'

        rgb = safe_load_image(rgb_path)
        rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)
        depth_raw = cv2.imread(str(depth_path), cv2.IMREAD_ANYDEPTH)
        if depth_raw is None:
            depth_raw = np.zeros((rgb.shape[0], rgb.shape[1]), dtype=np.float32)

        depth = depth_to_meters(depth_raw)  # meters

        # Crop ROI
        x, y, w, h = self.roi['x'], self.roi['y'], self.roi['width'], self.roi['height']
        rgb_roi = rgb[y:y+h, x:x+w]
        depth_roi = depth[y:y+h, x:x+w]

        # GT mask handling (if available)
        if 'mask' in self.df.columns and isinstance(row['mask'], str):
            mask_path = self.data_dir / 'masks' / row['mask']
            mask = cv2.imread(str(mask_path), 0)
            mask = mask[y:y+h, x:x+w] if mask is not None else np.zeros((h, w), dtype=np.uint8)
        else:
            gt_z = row.get('z', np.nan)
            if np.isfinite(gt_z):
                z_thresh = 0.06
                valid = (depth_roi > 0) & (np.abs(depth_roi - gt_z) < z_thresh)
                mask = (valid).astype(np.uint8)
            else:
                mask = np.zeros((h, w), dtype=np.uint8)

        # Per-sample depth normalization: subtract median
        depth_med = np.median(depth_roi[depth_roi > 0]) if np.any(depth_roi > 0) else 0.0
        depth_norm = depth_roi.copy().astype(np.float32)
        if depth_med > 0:
            depth_norm = depth_norm - depth_med

        # Stack RGB + depth channel (depth scaled to approx same dynamic range)
        image = np.dstack([rgb_roi, depth_norm[..., None]]).astype(np.float32) / 255.0

        if self.transform:
            transformed = self.transform(image=image, mask=mask)
            image_t = transformed['image']
            mask_t = transformed['mask'].long()
        else:
            image_t = torch.from_numpy(image.transpose(2, 0, 1)).float()
            mask_t = torch.from_numpy(mask).long()

        center = np.array([row.get('x', 0.0), row.get('y', 0.0), row.get('z', 0.0)], dtype=np.float32)
        normal = np.array([row.get('Rx', 0.0), row.get('Ry', 0.0), row.get('Rz', 1.0)], dtype=np.float32)
        normal = normal / (np.linalg.norm(normal) + 1e-8)

        depth_roi_tensor = torch.from_numpy(depth_roi.astype(np.float32))  # meters

        return {
            'image': image_t,             # (4, H, W)
            'mask': mask_t,               # (H, W)
            'center': torch.from_numpy(center).float(),
            'normal': torch.from_numpy(normal).float(),
            'depth': depth_roi_tensor,    # (H, W) meters
            'id': int(row['id'])
        }

# ====================================================================
# MODEL PARTS
# ====================================================================
class DepthEncoder(nn.Module):
    def __init__(self, out_dim=768):
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv2d(1, 32, 3, padding=1),
            nn.BatchNorm2d(32),
            nn.GELU(),
            nn.MaxPool2d(2),

            nn.Conv2d(32, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.GELU(),
            nn.MaxPool2d(2),

            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.GELU(),
            nn.AdaptiveAvgPool2d((1, 1))
        )
        self.fc = nn.Linear(128, out_dim)

    def forward(self, d):
        x = self.net(d)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

def inflate_conv1_to_4_channels(model, new_in_ch=4):
    if hasattr(model, 'patch_embed') and hasattr(model.patch_embed, 'proj'):
        conv = model.patch_embed.proj
    elif hasattr(model, 'conv_proj'):
        conv = model.conv_proj
    else:
        raise RuntimeError("Cannot find patch projection conv in model")

    w = conv.weight
    if w.shape[1] == new_in_ch:
        return model

    new_w = torch.zeros(w.shape[0], new_in_ch, w.shape[2], w.shape[3], device=w.device)
    in_ch_old = w.shape[1]
    copy_ch = min(in_ch_old, 3)
    new_w[:, :copy_ch, :, :] = w[:, :copy_ch, :, :]

    if new_in_ch > copy_ch:
        mean_rgb = w[:, :copy_ch, :, :].mean(dim=1, keepdim=True)
        for i in range(copy_ch, new_in_ch):
            new_w[:, i:i+1, :, :] = mean_rgb

    conv.in_channels = new_in_ch
    conv.weight = nn.Parameter(new_w)
    return model

class ParcelNetOptimized(nn.Module):
    def __init__(self, config: Config):
        super().__init__()
        self.vit = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=0, global_pool='')
        self.vit = inflate_conv1_to_4_channels(self.vit, new_in_ch=config.NUM_CHANNELS)

        embed_dim = self.vit.embed_dim
        self.depth_encoder = DepthEncoder(out_dim=embed_dim)

        # segmentation decoder
        self.seg_decoder = nn.Sequential(
            nn.Conv2d(embed_dim, 512, 3, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),
            nn.Conv2d(512, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),
            nn.Conv2d(256, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),
            nn.Conv2d(128, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),
            nn.Conv2d(64, config.NUM_CLASSES, kernel_size=1)
        )

        # pose heads using CLS token + depth token
        self.center_head = nn.Sequential(
            nn.Linear(embed_dim * 2, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            nn.Linear(512, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            nn.Linear(256, 3)
        )
        self.normal_head = nn.Sequential(
            nn.Linear(embed_dim * 2, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            nn.Linear(512, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            nn.Linear(256, 3)
        )
        # quaternion auxiliary head (4 dims, normalized)
        self.quat_head = nn.Sequential(
            nn.Linear(embed_dim * 2, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            nn.Linear(256, 4)
        )

    def forward(self, x):
        B = x.size(0)
        feats = self.vit.forward_features(x)
        cls_token = feats[:, 0, :]
        tokens = feats[:, 1:, :]
        N = tokens.shape[1]
        D = tokens.shape[2]

        depth = x[:, 3:4, :, :]
        depth_token = self.depth_encoder(depth)
        depth_token_norm = F.normalize(depth_token, dim=1)

        pose_token = torch.cat([cls_token, depth_token], dim=1)

        H_patch = W_patch = int(math.sqrt(N))
        seg_feat = tokens.transpose(1, 2).reshape(B, D, H_patch, W_patch)
        seg_logits = self.seg_decoder(seg_feat)

        center = self.center_head(pose_token)
        normal = self.normal_head(pose_token)
        normal = F.normalize(normal, dim=1)

        quat = self.quat_head(pose_token)
        quat = quat / (torch.norm(quat, dim=1, keepdim=True) + 1e-8)  # unit quaternion approx

        return seg_logits, center, normal, quat

# ====================================================================
# LOSSES & METRICS
# ====================================================================
class DiceLoss(nn.Module):
    def __init__(self, smooth=1e-5):
        super().__init__()
        self.smooth = smooth

    def forward(self, logits, targets):
        probs = torch.softmax(logits, dim=1)[:, 1]
        targets_f = (targets == 1).float()
        inter = (probs * targets_f).sum(dim=(1, 2))
        union = probs.sum(dim=(1, 2)) + targets_f.sum(dim=(1, 2))
        dice = (2 * inter + self.smooth) / (union + self.smooth)
        return 1 - dice.mean()

class FocalLoss(nn.Module):
    def __init__(self, gamma=2.0, alpha=0.25):
        super().__init__()
        self.gamma = gamma
        self.alpha = alpha
        self.ce = nn.CrossEntropyLoss(reduction='none')

    def forward(self, logits, targets):
        B, C, H, W = logits.shape
        logits_flat = logits.permute(0, 2, 3, 1).reshape(-1, C)
        targets_flat = targets.view(-1)
        logpt = -self.ce(logits_flat, targets_flat)
        pt = torch.exp(logpt)
        focal = -((1 - pt) ** self.gamma) * logpt
        alpha_factor = torch.ones_like(targets_flat).float() * (1 - self.alpha)
        alpha_factor[targets_flat == 1] = self.alpha
        loss = (focal * alpha_factor).mean()
        return loss

# angular OE loss consistent with evaluation: theta = acos(|n . n_hat|) in radians -> returned as mean degrees optionally
def oe_angular_loss_in_radians(pred_normals, gt_normals):
    # pred_normals, gt_normals: (B,3) normalized
    dot = (pred_normals * gt_normals).sum(dim=1)
    dot_abs = dot.abs().clamp(-1.0 + 1e-7, 1.0 - 1e-7)
    ang = torch.acos(dot_abs)  # radians
    return ang.mean()  # radians

# convert quaternion to direction vector: rotate canonical axis (0,0,1) by quaternion to get surface normal direction
def quaternion_to_direction(quat: torch.Tensor, axis: torch.Tensor = None):
    """
    quat: (B,4) unit quaternions [w, x, y, z] or arbitrary ordering.
    We will assume quat provided as (q0,q1,q2,q3) and treat q0 as scalar.
    axis: (3,) tensor; if None use z-axis [0,0,1]
    returns: (B,3) direction vectors (unit)
    """
    if axis is None:
        axis = torch.tensor([0.0, 0.0, 1.0], device=quat.device, dtype=quat.dtype)

    # normalize quat
    q = quat / (quat.norm(dim=1, keepdim=True) + 1e-8)
    q0 = q[:, 0:1]; qv = q[:, 1:4]  # scalar, vector
    # rotate axis: v' = v + 2*q0*(qv x v) + 2*(qv x (qv x v))
    v = axis.view(1, 3).repeat(q.shape[0], 1)
    qv_cross_v = torch.cross(qv, v, dim=1)
    qv_cross_qv_cross_v = torch.cross(qv, qv_cross_v, dim=1)
    v_rot = v + 2.0 * q0 * qv_cross_v + 2.0 * qv_cross_qv_cross_v
    v_rot = F.normalize(v_rot, dim=1)
    return v_rot

class CombinedLossOE(nn.Module):
    def __init__(self, cfg: Config):
        super().__init__()
        self.focal = FocalLoss()
        self.dice = DiceLoss()
        self.mse = nn.MSELoss()
        self.l1 = nn.L1Loss()
        self.cfg = cfg

    def forward(self, pred_mask_logits, pred_center, pred_normal, pred_quat, gt_mask, gt_center, gt_normal, depth_perpixel_normals=None):
        # segmentation
        loss_focal = self.focal(pred_mask_logits, gt_mask)
        loss_dice = self.dice(pred_mask_logits, gt_mask)
        loss_seg = loss_focal + loss_dice

        # center (same as before)
        loss_center_mse = self.mse(pred_center, gt_center)
        loss_center_l1 = self.l1(pred_center, gt_center)
        loss_center = 0.6 * loss_center_mse + 0.4 * loss_center_l1

        # normal OE loss: sign-invariant angular distance (radians)
        pred_normal_u = F.normalize(pred_normal, dim=1)
        gt_normal_u = F.normalize(gt_normal, dim=1)
        loss_normal_gt = oe_angular_loss_in_radians(pred_normal_u, gt_normal_u)  # rad

        # depth per-pixel normal guidance aggregated inside mask if provided
        loss_depth = torch.tensor(0.0, device=pred_normal.device)
        if depth_perpixel_normals is not None:
            # depth_perpixel_normals expected shape (B,3,H,W)
            # compute per-sample median inside mask or global median
            # we'll accept depth_perpixel_normals as (B,3,H,W) and corresponding mask from outer scope by aggregating
            # here the caller should have aggregated; allow direct pass as (B,3)
            if depth_perpixel_normals.ndim == 2 or depth_perpixel_normals.ndim == 1:
                # aggregated already (B,3)
                depth_agg = F.normalize(depth_perpixel_normals.to(pred_normal.device), dim=1)
                cos = (pred_normal_u * depth_agg).sum(dim=1).clamp(-1.0 + 1e-7, 1.0 - 1e-7)
                # sign-invariant guidance
                loss_depth = (1.0 - cos.abs()).mean()
            else:
                # fallback: if given per-pixel, aggregate median across H*W
                B = pred_normal.shape[0]
                dp = depth_perpixel_normals.view(B, 3, -1)  # (B,3,N)
                med = dp.median(dim=2).values
                med = F.normalize(med, dim=1)
                cos = (pred_normal_u * med.to(pred_normal.device)).sum(dim=1).clamp(-1.0 + 1e-7, 1.0 - 1e-7)
                loss_depth = (1.0 - cos.abs()).mean()

        # quaternion consistency: convert quaternion to direction (rotate z-axis)
        quat_dir = quaternion_to_direction(pred_quat)  # (B,3)
        quat_dir_u = F.normalize(quat_dir, dim=1)
        cos_q = (pred_normal_u * quat_dir_u).sum(dim=1).clamp(-1.0 + 1e-7, 1.0 - 1e-7)
        # use sign-invariant consistency: 1 - |dot|
        loss_quat_consistency = (1.0 - cos_q.abs()).mean()

        # total normal loss: weight GT angular (rad) plus 1-|dot| guidance from depth and quat
        # convert rad-> unitless by dividing by pi/2 (max angle ~90deg) to roughly scale, or leave rad directly and scale weights
        loss_normal = loss_normal_gt + self.cfg.DEPTH_NORMAL_WEIGHT * loss_depth + self.cfg.QUAT_LOSS_WEIGHT * loss_quat_consistency

        total = (self.cfg.WEIGHT_SEG * loss_seg +
                 self.cfg.WEIGHT_CENTER * loss_center +
                 self.cfg.WEIGHT_NORMAL * loss_normal)
        breakdown = {
            'seg': float(loss_seg.item()),
            'center': float(loss_center.item()),
            'normal_rad': float(loss_normal.item())
        }
        return total, breakdown

# ====================================================================
# DEPTH -> PER-PIXEL NORMALS (torch)
# ====================================================================
def compute_perpixel_normals_from_depth(depth: torch.Tensor, K: np.ndarray):
    """
    Compute approximate normals from depth map using spatial gradients in image space,
    then rotate them into camera coordinates using intrinsics approximations if needed.
    depth: (B, H, W) in meters
    returns: (B,3,H,W) normals (unit)
    """
    device = depth.device
    dtype = depth.dtype
    B, H, W = depth.shape
    # Sobel kernels
    kx = torch.tensor([[-1., 0., 1.], [-2., 0., 2.], [-1., 0., 1.]], device=device, dtype=dtype).view(1,1,3,3)/8.0
    ky = torch.tensor([[-1., -2., -1.], [0.,0.,0.], [1.,2.,1.]], device=device, dtype=dtype).view(1,1,3,3)/8.0

    d = depth.unsqueeze(1)  # (B,1,H,W)
    pad = (1,1,1,1)
    dzdx = F.conv2d(F.pad(d, pad, mode='replicate'), kx).squeeze(1)  # (B,H,W)
    dzdy = F.conv2d(F.pad(d, pad, mode='replicate'), ky).squeeze(1)

    nx = -dzdx
    ny = -dzdy
    nz = torch.ones_like(nx)
    norm = torch.sqrt(nx*nx + ny*ny + nz*nz + 1e-8)
    nx = nx / norm; ny = ny / norm; nz = nz / norm

    normals = torch.stack([nx, ny, nz], dim=1)  # (B,3,H,W)
    normals = F.normalize(normals.view(B,3,-1), dim=1).view(B,3,H,W)
    return normals

# ====================================================================
# TRANSFORMS
# ====================================================================
def get_transforms(train=True):
    if train:
        return A.Compose([
            A.Resize(Config.IMAGE_SIZE, Config.IMAGE_SIZE),
            A.OneOf([A.HorizontalFlip(p=0.5), A.VerticalFlip(p=0.2)], p=0.5),
            A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5, border_mode=cv2.BORDER_CONSTANT),
            A.RandomBrightnessContrast(p=0.5),
            A.GaussNoise(p=0.2),
            A.CoarseDropout(max_holes=1, max_height=20, max_width=20, min_holes=1, min_height=5, min_width=5, p=0.3),
            A.Normalize(mean=[0.485, 0.456, 0.406, 0.0], std=[0.229, 0.224, 0.225, 1.0]),
            ToTensorV2()
        ])
    else:
        return A.Compose([
            A.Resize(Config.IMAGE_SIZE, Config.IMAGE_SIZE),
            A.Normalize(mean=[0.485, 0.456, 0.406, 0.0], std=[0.229, 0.224, 0.225, 1.0]),
            ToTensorV2()
        ])

# ====================================================================
# TRAIN / VALID LOOPS (OE metrics included)
# ====================================================================
def unscale_mask_logits(pred_logits, target_size):
    if pred_logits.shape[-2:] != target_size:
        return F.interpolate(pred_logits, size=target_size, mode='bilinear', align_corners=False)
    return pred_logits
def train_one_epoch(model, loader, criterion, optimizer, scaler, cfg, scheduler=None):
    model.train()
    running_loss = 0.0
    metrics = {'seg':0.0, 'center':0.0, 'normal_rad':0.0, 'iou':0.0, 'oe_deg':0.0, 'oe_norm':0.0}
    n = 0
    pbar = tqdm(loader, desc='Train', leave=False)
    for step, batch in enumerate(pbar):
        images = batch['image'].to(cfg.DEVICE)           # (B,4,H,W) after transform
        masks = batch['mask'].to(cfg.DEVICE)             # (B,H,W) after transform
        centers = batch['center'].to(cfg.DEVICE)
        normals = batch['normal'].to(cfg.DEVICE)
        depths = batch['depth'].to(cfg.DEVICE)           # (B, H_raw, W_raw) raw ROI before transform

        with torch.cuda.amp.autocast(enabled=cfg.AMP):
            pred_logits, pred_centers, pred_normals, pred_quat = model(images)
            H, W = masks.shape[1], masks.shape[2]
            pred_logits = unscale_mask_logits(pred_logits, (H, W))

            # --- RESIZE DEPTH to transform resolution so mask and depth align ---
            # depths: (B, H_raw, W_raw) -> (B,1,H,W) -> squeeze -> (B,H,W)
            depths_unsq = depths.unsqueeze(1)  # (B,1,H_raw,W_raw)
            depths_resized = F.interpolate(depths_unsq, size=(H, W), mode='bilinear', align_corners=False).squeeze(1)

            # compute per-pixel normals on resized depth
            perpix_normals = compute_perpixel_normals_from_depth(depths_resized, cfg.K)  # (B,3,H,W)

            # aggregate per-sample median inside mask (mask matches H,W now)
            B = depths_resized.shape[0]
            depth_guidance = []
            for i in range(B):
                mask_i = masks[i].bool()                      # (H,W)
                normals_i = perpix_normals[i].view(3, -1)     # (3, H*W)
                mask_flat = mask_i.view(-1)                   # (H*W)
                if mask_flat.any():
                    # select masked pixels; ensure there are masked pixels
                    try:
                        normals_masked = normals_i[:, mask_flat]
                        if normals_masked.shape[1] > 0:
                            med = normals_masked.median(dim=1).values
                        else:
                            med = normals_i.median(dim=1).values
                    except IndexError:
                        # fallback to global median if indexing fails for any reason
                        med = normals_i.median(dim=1).values
                else:
                    med = normals_i.median(dim=1).values
                depth_guidance.append(med)
            depth_guidance = torch.stack(depth_guidance, dim=0)  # (B,3)

            loss, breakdown = criterion(pred_logits, pred_centers, pred_normals, pred_quat, masks, centers, normals, depth_guidance)

        loss = loss / cfg.GRAD_ACCUM_STEPS
        scaler.scale(loss).backward()

        if (step + 1) % cfg.GRAD_ACCUM_STEPS == 0:
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.GRAD_CLIP)
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()

        # scheduler per-step when using OneCycle
        if scheduler is not None:
            scheduler.step()

        running_loss += (loss.item() * cfg.GRAD_ACCUM_STEPS)

        # metrics: IoU and OE
        with torch.no_grad():
            probs = torch.softmax(pred_logits, dim=1)[:,1,:,:].detach().cpu().numpy()
            gt = masks.detach().cpu().numpy()
            batch_iou = 0.0
            for i in range(probs.shape[0]):
                batch_iou += compute_mask_iou(probs[i], gt[i])
            batch_iou /= probs.shape[0]

            pred_n = pred_normals.detach().cpu().numpy()
            gt_n = normals.detach().cpu().numpy()
            oe_deg, oe_norm = orientation_error_deg_and_normalized(pred_n, gt_n, theta_max_deg=cfg.THETA_MAX_DEG)

        metrics['seg'] += breakdown['seg']
        metrics['center'] += breakdown['center']
        metrics['normal_rad'] += breakdown['normal_rad']
        metrics['iou'] += batch_iou
        metrics['oe_deg'] += oe_deg
        metrics['oe_norm'] += oe_norm
        n += 1

        pbar.set_postfix({'loss': running_loss / n, 'iou': metrics['iou'] / n, 'oe_deg': metrics['oe_deg']/n})

    return running_loss / n, {k: v / n for k, v in metrics.items()}


@torch.no_grad()
def validate_one_epoch(model, loader, criterion, cfg):
    model.eval()
    running_loss = 0.0
    metrics = {'seg':0.0, 'center':0.0, 'normal_rad':0.0, 'iou':0.0, 'oe_deg':0.0, 'oe_norm':0.0}
    n = 0
    for batch in tqdm(loader, desc='Val', leave=False):
        images = batch['image'].to(cfg.DEVICE)
        masks = batch['mask'].to(cfg.DEVICE)
        centers = batch['center'].to(cfg.DEVICE)
        normals = batch['normal'].to(cfg.DEVICE)
        depths = batch['depth'].to(cfg.DEVICE)

        pred_logits, pred_centers, pred_normals, pred_quat = model(images)
        H, W = masks.shape[1], masks.shape[2]
        pred_logits = unscale_mask_logits(pred_logits, (H, W))

        # resize depth to match mask/image resolution before computing normals
        depths_resized = F.interpolate(depths.unsqueeze(1), size=(H, W), mode='bilinear', align_corners=False).squeeze(1)
        perpix_normals = compute_perpixel_normals_from_depth(depths_resized, cfg.K)

        B = depths_resized.shape[0]
        depth_guidance = []
        for i in range(B):
            mask_i = masks[i].bool()
            normals_i = perpix_normals[i].view(3, -1)
            mask_flat = mask_i.view(-1)
            if mask_flat.any():
                try:
                    normals_masked = normals_i[:, mask_flat]
                    if normals_masked.shape[1] > 0:
                        med = normals_masked.median(dim=1).values
                    else:
                        med = normals_i.median(dim=1).values
                except IndexError:
                    med = normals_i.median(dim=1).values
            else:
                med = normals_i.median(dim=1).values
            depth_guidance.append(med)
        depth_guidance = torch.stack(depth_guidance, dim=0)

        loss, breakdown = criterion(pred_logits, pred_centers, pred_normals, pred_quat, masks, centers, normals, depth_guidance)

        running_loss += loss.item()

        probs = torch.softmax(pred_logits, dim=1)[:,1,:,:].detach().cpu().numpy()
        gt = masks.detach().cpu().numpy()
        batch_iou = 0.0
        for i in range(probs.shape[0]):
            batch_iou += compute_mask_iou(probs[i], gt[i])
        batch_iou /= probs.shape[0]

        pred_n = pred_normals.detach().cpu().numpy()
        gt_n = normals.detach().cpu().numpy()
        oe_deg, oe_norm = orientation_error_deg_and_normalized(pred_n, gt_n, theta_max_deg=cfg.THETA_MAX_DEG)

        metrics['seg'] += breakdown['seg']
        metrics['center'] += breakdown['center']
        metrics['normal_rad'] += breakdown['normal_rad']
        metrics['iou'] += batch_iou
        metrics['oe_deg'] += oe_deg
        metrics['oe_norm'] += oe_norm
        n += 1

    return running_loss / n, {k: v / n for k, v in metrics.items()}

# ====================================================================
# TRAINING ENTRYPOINT
# ====================================================================
def train_model(csv_path=None):
    seed_everything()
    cfg = Config

    if csv_path is None:
        csv_path = cfg.TRAIN_DIR / 'Public train.csv'

    transforms_train = get_transforms(train=True)
    transforms_val = get_transforms(train=False)

    full_dataset = ParcelDataset(cfg.TRAIN_DIR, csv_path, transform=transforms_train, roi=cfg.ROI)
    n_total = len(full_dataset)
    n_train = int(0.8 * n_total)
    n_val = n_total - n_train
    train_set, val_set = torch.utils.data.random_split(full_dataset, [n_train, n_val], generator=torch.Generator().manual_seed(cfg.SEED))
    val_set.dataset.transform = transforms_val

    train_loader = DataLoader(train_set, batch_size=cfg.BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)
    val_loader = DataLoader(val_set, batch_size=cfg.BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)

    model = ParcelNetOptimized(cfg).to(cfg.DEVICE)
    print("Model parameters: {:.2f}M".format(sum(p.numel() for p in model.parameters()) / 1e6))

    criterion = CombinedLossOE(cfg)

    param_groups = [
        {"params": [p for n,p in model.named_parameters() if p.requires_grad and ('bias' in n or 'norm' in n)], "weight_decay": 0.0},
        {"params": [p for n,p in model.named_parameters() if p.requires_grad and not ('bias' in n or 'norm' in n)], "weight_decay": cfg.WEIGHT_DECAY},
    ]
    optimizer = torch.optim.AdamW(param_groups, lr=cfg.LR)

    if cfg.USE_ONECYCLE:
        steps_per_epoch = len(train_loader)
        total_steps = cfg.NUM_EPOCHS * steps_per_epoch
        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=cfg.MAX_LR, total_steps=total_steps, pct_start=cfg.ONECYCLE_PCT_START)
    else:
        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)

    scaler = torch.cuda.amp.GradScaler(enabled=cfg.AMP)

    best_oe = float('inf')  # lower better (normalized OE)
    patience = 6
    patience_count = 0

    for epoch in range(cfg.NUM_EPOCHS):
        print(f"\n=== Epoch {epoch+1}/{cfg.NUM_EPOCHS} ===")
        train_set.dataset.transform = transforms_train
        train_loss, train_metrics = train_one_epoch(model, train_loader, criterion, optimizer, scaler, cfg, scheduler=(scheduler if cfg.USE_ONECYCLE else None))

        if not cfg.USE_ONECYCLE:
            scheduler.step()

        val_loss, val_metrics = validate_one_epoch(model, val_loader, criterion, cfg)

        print(f"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}")
        print(f"Train IoU: {train_metrics['iou']:.4f} | Val IoU: {val_metrics['iou']:.4f}")
        print(f"Train OE (deg/norm): {train_metrics['oe_deg']:.3f} / {train_metrics['oe_norm']:.4f}")
        print(f"Val   OE (deg/norm): {val_metrics['oe_deg']:.3f} / {val_metrics['oe_norm']:.4f}")
        print(f"Train (seg,center,normal_rad): {train_metrics['seg']:.4f}, {train_metrics['center']:.4f}, {train_metrics['normal_rad']:.4f}")
        print(f"Val   (seg,center,normal_rad): {val_metrics['seg']:.4f}, {val_metrics['center']:.4f}, {val_metrics['normal_rad']:.4f}")

        # primary checkpointing by normalized OE (lower better)
        if val_metrics['oe_norm'] < best_oe:
            best_oe = val_metrics['oe_norm']
            torch.save({'model_state': model.state_dict(), 'optimizer_state': optimizer.state_dict()}, cfg.BEST_MODEL)
            print("Saved best model by OE.")
            patience_count = 0
        else:
            patience_count += 1
            if patience_count >= patience:
                print("Early stopping triggered on OE.")
                break

    print("Training finished. Best normalized OE: {:.4f}".format(best_oe))
    return model

# ====================================================================
# INFERENCE helpers (unchanged semantics; outputs Rx,Ry,Rz as normal vector)
# ====================================================================
def mask_to_3d_centroid(mask_prob: np.ndarray, depth: np.ndarray, K: np.ndarray, roi: Dict):
    mask_bin = (mask_prob >= 0.4).astype(np.uint8)
    ys, xs = np.where(mask_bin == 1)
    if len(xs) == 0:
        return None

    zs = depth[ys, xs]
    valid = zs > 0
    if valid.sum() == 0:
        return None

    xs = xs[valid]; ys = ys[valid]; zs = zs[valid]
    z_med = np.median(zs)
    inliers = np.abs(zs - z_med) < 0.2
    xs = xs[inliers]; ys = ys[inliers]; zs = zs[inliers]
    if len(xs) == 0:
        return None

    fx, fy = K[0,0], K[1,1]
    cx, cy = K[0,2], K[1,2]
    X = (xs + roi['x'] - cx) * zs / fx
    Y = (ys + roi['y'] - cy) * zs / fy
    Z = zs
    centroid = np.array([X.mean(), Y.mean(), Z.mean()], dtype=np.float32)
    return centroid

def inference(model_path=None, test_dir=None, save_csv='/content/Submission_3D.csv'):
    cfg = Config
    if model_path is None:
        model_path = cfg.BEST_MODEL
    if test_dir is None:
        test_dir = cfg.BASE_DIR / 'private'

    device = cfg.DEVICE
    checkpoint = torch.load(model_path, map_location=device)
    model = ParcelNetOptimized(cfg).to(device)
    model.load_state_dict(checkpoint['model_state'])
    model.eval()

    transform = get_transforms(train=False)
    roi = cfg.ROI
    test_rgb_files = sorted(list((test_dir / 'rgb').glob('*.png')))
    results = []
    with torch.no_grad():
        for f in tqdm(test_rgb_files, desc='Infer'):
            sample_id = f.stem
            rgb = safe_load_image(f)
            rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)
            depth_raw = cv2.imread(str(test_dir / 'depth' / f'{sample_id}.png'), cv2.IMREAD_ANYDEPTH)
            if depth_raw is None:
                depth_raw = np.zeros((rgb.shape[0], rgb.shape[1]), dtype=np.float32)
            depth = depth_to_meters(depth_raw)

            x, y, w, h = roi['x'], roi['y'], roi['width'], roi['height']
            rgb_roi = rgb[y:y+h, x:x+w]
            depth_roi = depth[y:y+h, x:x+w]
            depth_med = np.median(depth_roi[depth_roi > 0]) if np.any(depth_roi > 0) else 0.0
            depth_norm = (depth_roi - depth_med).astype(np.float32)
            image = np.dstack([rgb_roi, depth_norm[..., None]]).astype(np.float32) / 255.0
            transformed = transform(image=image)
            image_tensor = transformed['image'].unsqueeze(0).to(device)

            pred_logits, pred_center, pred_normal, pred_quat = model(image_tensor)
            pred_logits = unscale_mask_logits(pred_logits, (h, w))
            pred_prob = torch.softmax(pred_logits, dim=1)[0, 1].cpu().numpy()

            centroid = mask_to_3d_centroid(pred_prob, depth_roi, cfg.K, roi)
            if centroid is None:
                center = pred_center[0].cpu().numpy()
            else:
                center = centroid

            normal = pred_normal[0].cpu().numpy()
            normal = normal / (np.linalg.norm(normal) + 1e-8)

            results.append({
                'id': int(sample_id),
                'x': float(center[0]),
                'y': float(center[1]),
                'z': float(center[2]),
                'Rx': float(normal[0]),
                'Ry': float(normal[1]),
                'Rz': float(normal[2])
            })

    df = pd.DataFrame(results)
    df = df.sort_values('id')
    df['image_filename'] = df['id'].apply(lambda x: f'image_{x:04d}.png')
    df = df[['image_filename', 'x', 'y', 'z', 'Rx', 'Ry', 'Rz']]
    df.to_csv(save_csv, index=False)
    print("Saved submission:", save_csv)
    print(df.head())
    return df

if __name__ == '__main__':
    csv_p = Config.TRAIN_DIR / 'Public train.csv'
    if not csv_p.exists():
        print("Train CSV not found:", csv_p)
    else:
        train_model(csv_p)

inference()